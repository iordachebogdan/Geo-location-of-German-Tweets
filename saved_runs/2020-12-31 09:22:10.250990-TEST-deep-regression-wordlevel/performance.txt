2020-12-31 09:22:08.739923: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Training model for LAT
2020-12-31 09:23:24.489932: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-31 09:23:24.490925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-12-31 09:23:24.523894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:24.524520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
2020-12-31 09:23:24.524572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2020-12-31 09:23:24.526141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2020-12-31 09:23:24.526218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2020-12-31 09:23:24.527905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-12-31 09:23:24.528268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-12-31 09:23:24.529701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-12-31 09:23:24.530507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2020-12-31 09:23:24.533426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2020-12-31 09:23:24.533549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:24.534121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:24.534820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-31 09:23:24.535316: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-31 09:23:24.535442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:24.535956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
2020-12-31 09:23:24.536006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2020-12-31 09:23:24.536050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2020-12-31 09:23:24.536073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2020-12-31 09:23:24.536104: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-12-31 09:23:24.536122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-12-31 09:23:24.536142: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-12-31 09:23:24.536163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2020-12-31 09:23:24.536185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2020-12-31 09:23:24.536256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:24.536804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:24.537310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-31 09:23:24.537363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2020-12-31 09:23:25.270762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-31 09:23:25.270828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2020-12-31 09:23:25.270844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2020-12-31 09:23:25.271040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:25.271659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:25.272245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-31 09:23:25.272737: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-12-31 09:23:25.272786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13960 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
Fitting text vectorizer...
Building model...
2020-12-31 09:23:28.324005: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-12-31 09:23:28.324427: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz
Epoch 1/20
2020-12-31 09:23:33.285515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2020-12-31 09:23:33.835441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
1412/1412 [==============================] - 240s 164ms/step - loss: 2.8763 - val_loss: 0.8094

Epoch 00001: val_loss improved from inf to 0.80936, saving model to checkpoints/best.h5
Epoch 2/20
1412/1412 [==============================] - 227s 161ms/step - loss: 1.0790 - val_loss: 0.7033

Epoch 00002: val_loss improved from 0.80936 to 0.70327, saving model to checkpoints/best.h5
Epoch 3/20
1412/1412 [==============================] - 224s 158ms/step - loss: 0.9490 - val_loss: 0.5688

Epoch 00003: val_loss improved from 0.70327 to 0.56876, saving model to checkpoints/best.h5
Epoch 4/20
1412/1412 [==============================] - 223s 158ms/step - loss: 0.7037 - val_loss: 1.0050

Epoch 00004: val_loss did not improve from 0.56876
Epoch 5/20
1412/1412 [==============================] - 220s 156ms/step - loss: 0.6418 - val_loss: 0.5476

Epoch 00005: val_loss improved from 0.56876 to 0.54761, saving model to checkpoints/best.h5
Epoch 6/20
1412/1412 [==============================] - 220s 156ms/step - loss: 0.5756 - val_loss: 0.9005

Epoch 00006: val_loss did not improve from 0.54761
Epoch 7/20
1412/1412 [==============================] - 220s 156ms/step - loss: 0.5451 - val_loss: 1.2317

Epoch 00007: val_loss did not improve from 0.54761
Epoch 8/20
1412/1412 [==============================] - 218s 154ms/step - loss: 0.5536 - val_loss: 0.5333

Epoch 00008: val_loss improved from 0.54761 to 0.53330, saving model to checkpoints/best.h5
Epoch 9/20
1412/1412 [==============================] - 220s 156ms/step - loss: 0.4615 - val_loss: 0.7156

Epoch 00009: val_loss did not improve from 0.53330
Epoch 10/20
1412/1412 [==============================] - 219s 155ms/step - loss: 0.4236 - val_loss: 0.6404

Epoch 00010: val_loss did not improve from 0.53330
Epoch 11/20
1412/1412 [==============================] - 218s 155ms/step - loss: 0.4406 - val_loss: 0.5172

Epoch 00011: val_loss improved from 0.53330 to 0.51720, saving model to checkpoints/best.h5
Epoch 12/20
1412/1412 [==============================] - 220s 156ms/step - loss: 0.4215 - val_loss: 0.5207

Epoch 00012: val_loss did not improve from 0.51720
Epoch 13/20
1412/1412 [==============================] - 218s 155ms/step - loss: 0.3734 - val_loss: 0.5570

Epoch 00013: val_loss did not improve from 0.51720
Epoch 14/20
1412/1412 [==============================] - 219s 155ms/step - loss: 0.3607 - val_loss: 0.5524

Epoch 00014: val_loss did not improve from 0.51720
Epoch 15/20
1412/1412 [==============================] - 220s 156ms/step - loss: 0.3392 - val_loss: 0.7813

Epoch 00015: val_loss did not improve from 0.51720
Epoch 16/20
1412/1412 [==============================] - 222s 157ms/step - loss: 0.3467 - val_loss: 0.9423

Epoch 00016: val_loss did not improve from 0.51720
Epoch 00016: early stopping
1412/1412 [==============================] - 25s 16ms/step
191/191 [==============================] - 5s 16ms/step
197/197 [==============================] - 5s 16ms/step
Training model for LONG
Fitting text vectorizer...
Building model...
Epoch 1/20
1412/1412 [==============================] - 237s 164ms/step - loss: 1.4932 - val_loss: 0.8441

Epoch 00001: val_loss improved from inf to 0.84407, saving model to checkpoints/best.h5
Epoch 2/20
1412/1412 [==============================] - 226s 160ms/step - loss: 0.6460 - val_loss: 0.6150

Epoch 00002: val_loss improved from 0.84407 to 0.61500, saving model to checkpoints/best.h5
Epoch 3/20
1412/1412 [==============================] - 227s 160ms/step - loss: 0.5045 - val_loss: 0.6738

Epoch 00003: val_loss did not improve from 0.61500
Epoch 4/20
1412/1412 [==============================] - 224s 159ms/step - loss: 0.4173 - val_loss: 0.5795

Epoch 00004: val_loss improved from 0.61500 to 0.57948, saving model to checkpoints/best.h5
Epoch 5/20
1412/1412 [==============================] - 226s 160ms/step - loss: 0.3533 - val_loss: 0.5862

Epoch 00005: val_loss did not improve from 0.57948
Epoch 6/20
1412/1412 [==============================] - 223s 158ms/step - loss: 0.3131 - val_loss: 0.5556

Epoch 00006: val_loss improved from 0.57948 to 0.55564, saving model to checkpoints/best.h5
Epoch 7/20
1412/1412 [==============================] - 223s 158ms/step - loss: 0.2847 - val_loss: 0.6157

Epoch 00007: val_loss did not improve from 0.55564
Epoch 8/20
1412/1412 [==============================] - 222s 157ms/step - loss: 0.2503 - val_loss: 0.5505

Epoch 00008: val_loss improved from 0.55564 to 0.55049, saving model to checkpoints/best.h5
Epoch 9/20
1412/1412 [==============================] - 221s 157ms/step - loss: 0.2206 - val_loss: 0.5720

Epoch 00009: val_loss did not improve from 0.55049
Epoch 10/20
1412/1412 [==============================] - 221s 156ms/step - loss: 0.2051 - val_loss: 0.5812

Epoch 00010: val_loss did not improve from 0.55049
Epoch 11/20
1412/1412 [==============================] - 220s 156ms/step - loss: 0.1896 - val_loss: 0.5737

Epoch 00011: val_loss did not improve from 0.55049
Epoch 12/20
1412/1412 [==============================] - 221s 157ms/step - loss: 0.1787 - val_loss: 0.5686

Epoch 00012: val_loss did not improve from 0.55049
Epoch 13/20
1412/1412 [==============================] - 219s 155ms/step - loss: 0.1651 - val_loss: 0.5520

Epoch 00013: val_loss did not improve from 0.55049
Epoch 00013: early stopping
1412/1412 [==============================] - 25s 16ms/step
191/191 [==============================] - 5s 16ms/step
197/197 [==============================] - 5s 16ms/step
TRAIN: 0.2535045607436749
VAL: 0.533847085622374
